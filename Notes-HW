## Week 1 Homework Practice Exercises
    http://bcaffo.github.io/courses/06_StatisticalInference/homework/hw1.html#1
    
## Week 1 - Module 2 - Probability
  - Probability measures a population quantity that summarizes the randomness (conceptual thing that exists in the population that we would like to estimate). 
  - Probability operates on potential outcomes
  - Function assigned 0-1
  - P (A U B) cannot both occur; P(A) + P(B)
  - Probability that nothing occurs = 0; 1-P = probability something will occur
  - P(A U B) = P(A) + P(B) - P(A U B); can't just add probabilities if they have an interaction
  - Probability calculus is the foundation for probability - densitities and mass functions for random variables
      - Random variable is numeric outcome of experiment (discrete or continuous)
      - Probability Mass Function (PMF) - evaluated at a value corresponds to the probability that a random variable takes that value, to be a valid pmf, p must satisfiy: 1) not equal zero, 2) sum of possible values that a random variable can take must equal one; examples - Poisson, binomial, Bernoulli distrbutions
      - Probability Density Function (PDF): a function associated with continuous random variables; corresponds to to probabilities for that random variable. 
      - Cumulative distrbution function (CDF) of a random variable (X) returns the probability that the ranomd variable is less than or equal to the value (X) = F(x) = P(X =< x)
      - Survival function: S(x) = 1-x^2
      - Quantiles: a percentile is a quantile with an alpha expressed as a percent rather than a proportion. The population median is the 50th percentile. Percentiles ARE NOT probabilities. 
      -  A probability model connects data to a population using assumptions
      - A sample median is an estimator of a population median (the estimand).

## Week 1 - Module 3 - Conditional Probability
  - "What is the probability given partial information about what has occurred?"
  - If P(B) > 0, than the conditional probability of A is: P(A|B) = P(A ^ B)/P(B)
  - If A and B are unrelated, then the independent probability is: P(A|B) = P(A)P(B)/P(B) = P(A)
  - Baye's Rule: reverse the rule of the probability, we have P(B|A), but what is P(A|B)? Useful for diagnostics
          P(B|A) = P(A|B)P(B) / P(A|B)P(B) + P(A|Bc)P(Bc)
      - Sensitivity = P(+|D)
      - Specificity = P(-|D)
      - Positive Predictive Value = P(D|+)
      - Negative Predictive Value = P(Dc|-)
      - P(D|+) / P(Dc|+) = P(D)/(P(Dc) or Odds of disease
   - Independence - P(A|B) = P(A) where P(B)<0 - you cannot just multiply probabilities! 

## Week 1 - Module 4 - Expected Values
  - Expected values are very useful for characterizing populations and usually represent the first thing that we're interested in estimating.
  - Expected Values characterize a distribution, for example, the mean, which characterizes the center of a density or mass function. 
  - Expected values include: mean, variance, skewness
  - The center of mass is the empirical mean
  - Expected Values are: 
        - properties of populations
        - the population mean = center of population mass
        - sample mean is the center of mass of the observed data
        - the sample mean is an estimate of the population mean
        - the sample mean is unbiased, the population mean of its distribution is the mean that its trying to estimate
        - the more that goes into the same mean, the more concentrated its density, mass function is around the population mean. 

Random variables are said to be iid if they are ndependent and identically distributed. By independent we mean statistically unrelated from one another". Identically distributed means hat "all have been drawn from the same population distribution"

## Week 2 Practice Exercises
    http://bcaffo.github.io/courses/06_StatisticalInference/homework/hw2.html#1
    
    ## Variability
        The variance Var(x) = p(1-p)
        Sample variance: is the average squared variance - sample mean
      - The sample variance estimates the population variance.
      - The distribution of the sample variance is centered at what its estimating.
      - It gets more concentrated around the population variance with larger sample sizes.
      - The variance of the sample mean is the population variance divided by n.
      - The square root is the standard error.
      - It turns out that we can say a lot about the distribution of averages from random samples,
even though we only get one to look at in a given data set.
    
   ## Distributions 
      Most common distributions
      - Bernoulii distribution - binomial trials
        Bernoulli random variables are commonly used for modeling any binary trait for a random sample.
        So, for example, in a random sample whether or not a participant has high blood pressure would be
        reasonably modeled as Bernoulli
      - The normal distribution is so important that it is useful to memorize reference probabilities and
        quantiles. The image below shows reference lines at 0, 1, 2 and 3 standard deviations above and
        below 0. 
      - This is for the standard normal; however, all of the rules apply to non standard normals as 0, 1, 2 and 3 standard deviations above and below, the population mean.
            1. Approximately 68\%, 95\% and 99\% of the normal density lies within 1, 2 and 3 standard deviations from the mean, respectively.
            2. -1.28, -1.645, -1.96 and -2.33 are the 10th, 5th, 2:5th and 1st percentiles of the standard normal distribution, respectively.
            3. By symmetry, 1.28, 1.645, 1.96 and 2.33 are the 90th, 95th, 97:5th and 99th percentiles of the standard normal distribution, respectively.
      - The Poisson distribution is used to model counts. It is perhaps only second to the normal distribution usefulness. In fact, the Bernoulli, binomial and multinomial distributions can all be modeled by clever uses of the Poisson.
             The Poisson distribution is useful for rates, counts that occur over units of time. Specifically, if X  Poisson(t) where  = E[X/t] is the expected count per unit of time and t is the total monitoring time.
    
    ## Asymptotics
        - The CLT states that averages are approximately normal, with distributions.
                – centered at the population mean.
                – with standard deviation equal to the standard error of the mean.
                – CLT gives no guarantee that $n$ is large enough.
    
## Week 3 Practice Exercises
    http://bcaffo.github.io/courses/06_StatisticalInference/homework/hw3.html#1
    
  ## Confidence intervals
        The t distribution is useful for small sample size comparisons
        Normal distribution: Z ot t-test statistic
        
        
  ## Hypothesis Testing
        Type I error REJECTS a TRUE null hypothesis H_0
        Type II error ACCEPTS a FALSE null hypothesis H_0
        Bottom line: if you fail to reject the one sided test, you know that you will fail to reject the two sided.
    
  ## P-values
        The p-value is the probability under the null hypothesis of obtaining evidence as or more extreme than your test statistic (obtained from your observed data) in the direction of the alternative hypothesis.


## Week 4 Practice Exercises
    http://bcaffo.github.io/courses/06_StatisticalInference/homework/hw4.html#1








